---
Title: DBS101 Unit_six class6
categories: [DBS101 Unit_Six_class6]
tags: [DBS101]
---
# Introduction
Indexes are special data structures (often lookup tables) that a database uses to speed up the retrieval of records. Instead of scanning the entire table to find matching rows, the database search engine uses indexes to quickly locate the data. This greatly improves query performance, especially on large datasets.

## Types of indices
## Ordered Indices:
- Store values in sorted order; efficient for range and binary searches.

## Types of ordered indexing:
1. Clustering Index/Primary Index
- Records are stored in the same order as the index.
- Each entry has a search key and a pointer to the first matching record.
- The search key is often (but not always) the primary key.

2. Non-clustering Index/Secondary Index
- Records are not stored in the same order as the index.
- Each entry has a search key and pointers to all matching records.

- An index entry has a search key and pointers to matching records.
- A pointer includes the disk block ID and offset to locate the record.

## Types of Ordered Indices:
1. Dense Index: 
- Entry for every search key value.
- Clustering: Points to the first record with that key.
- Non-Clustering: Points to all records with that key.
2. Sparse Index: 
- Entry for only some search key values.
- Works only if data is sorted by the search key (clustering index).
- Each entry points to the first record with that key.

## Hashed Indices:
- Use a hash function to assign values to buckets; best for exact match queries.
## Search Key:
- An attribute or set of attributes used to look up records.

# Multilevel Indices
- An outer index on top of the main (inner) index.
- Inner index: Stored on disk.
- Outer index: Smaller, fits in memory for faster access.
![MI](/assets/unit6/multi-level-index.png)

## Searching with Multilevel Index:
- Binary search the in-memory outer index to find the range.
- Read and search only the relevant part of the inner index from disk.
### Usefulness:
- Great for large indexes that don’t fit in memory—speeds up searches by minimizing disk reads.

## Secondary Indices
- A secondary index is created on non-key attribute(s) of a table to speed up queries involving those columns. Unlike a primary index (on the primary key), a secondary index allows efficient searching and sorting without scanning the entire table. It's especially useful for frequently accessed non-key fields.

## B+ Tree
- A B+ tree is a self-balancing, height-balanced tree commonly used for indexing in databases. It maintains sorted data and allows efficient insertion, deletion, and search. All paths from the root to the leaves are of equal length.
- Non-leaf nodes (except the root) have between ⌈n/2⌉ and n children.
- The root has between 2 and n children.
- Leaf nodes contain actual data pointers; internal nodes store keys only.

## Self-Balancing Binary Search Tree
- A self-balancing binary search tree adjusts its structure during insertions and deletions to maintain minimal height, ensuring efficient operations.
![Selfbal](/assets/unit6/selfbal.png)

## Height-Balanced Tree
- A binary tree is height-balanced if the height difference between its left and right subtrees is at most 1.

## Why B Tree?
- B Trees were developed to improve access speed on physical storage like hard disks. Unlike binary, AVL, or red-black trees that store one key per node, B Trees store multiple keys per node, reducing tree height and improving access time for large datasets.
![btree](/assets/unit6/btree.png)

## Advantages of B Trees
- Store multiple keys per node
- Support multiple children per node
- Reduce tree height for faster disk access

## Properties of B Trees
- Keys in each node are sorted in increasing order.
- Leaf nodes are marked with a boolean flag.
- Internal nodes hold up to (n – 1) keys and n child pointers (for order n).
- Non-root nodes have between ⌈n/2⌉ and n children.
- All leaf nodes are at the same depth.
- The root has at least 2 children and at least 1 key.
- Tree height h ≥ logₜ((n + 1)/2), where t is the minimum degree.

## B+ Tree Overview
- A B+ Tree is a self-balancing, ordered M-way tree optimized for fast search, insertion, and deletion in O(log n) time. It supports efficient sequential access and is ideal for large block reads/writes.

## B+ Tree Properties
- Perfectly balanced: all leaf nodes at the same depth
- Every node (except root) is at least half full
- Each node stores M/2 – 1 to M – 1 keys
- An inner node with k keys has k + 1 children
![prop](/assets/unit6/pro.png)

## B+ Tree Nodes
- Each node contains a sorted array of key/value pairs:
- Keys come from the indexed attribute(s).
- Values differ by node type:
- Inner nodes store child pointers.
- Leaf nodes store actual data pointers.
- NULL keys are stored at the first or last leaf node.
![node](/assets/unit6/nodes1.png)
![node](/assets/unit6/node2.png)

## Hash Indices
- Use hashing to map search keys to buckets (storage units)
- Buckets can be in-memory (linked lists) or on-disk (blocks)
- Common in joins and in-memory databases

## How Hashing Works
- A hash function maps key K to a bucket: h(K)
- Records are inserted into the bucket’s list (overflow chaining if needed)
- Collisions (same hash for different keys) require checking all entries in the bucket

## Hash Index Operations
- Insert/Search/Delete: Use h(K) to find and access the bucket
- Supports only equality lookups, not range queries

## Disk-Based Hash Indices
- Records are stored in disk buckets.
- Overflow is handled by chaining extra buckets to the main one.
- Lookups search both the main and overflow buckets.

# Query Processing
- Query processing involves converting high-level queries (like SQL) into efficient operations that retrieve data from a database.

## Steps in Query Processing

1. Parsing and Translation
- Checks syntax and validates relations
- Converts SQL to a parse tree, then to relational algebra
- Handles views by substituting their definitions
- Produces a query evaluation plan: relational algebra + execution details (e.g., which index to use)

2. Evaluation
- The query execution engine runs the chosen plan step-by-step
- Executes operations in the specified order using the annotated methods

3. Optimization
- Chooses the most efficient execution plan
- Considers CPU, I/O cost, and data statistics
- May use pipelining: operators start execution without waiting for full input, reducing disk I/O and improving performance
![sql](/assets/unit6/spq.png)

3. Query Evaluation
- SQL is declarative—it states what to do, not how.
## To evaluate it:
- Convert SQL to relational algebra (which is procedural)
- Use a query execution plan to perform the operations efficiently
## How about subqueries
![how](/assets/unit6/how.png)
## Query Evaluation steps
![steps](/assets/unit6/steps.png)

## Query Optimization
- Query optimization selects the most efficient way to execute a query.
- Logical Plan: Uses relational algebra to represent the query
- Physical Plan: Adds details on how to execute each operation
- Specifies access methods (e.g., index scan)
![logical](/assets/unit6/logical.png)

## Physical Query Plan
- An annotated logical plan that includes:
- Access path: File scan or index usage
- Operator implementation: Specific algorithms
- Scheduling: Execution order and pipelining decisions
![physical](/assets/unit6/physical.png)

## Query Execution: Iterator Interface
- Each operator follows a simple interface with three methods:
- open(): Initializes state and parameters
- get_next(): Recursively fetches input, processes it, returns next output tuple
- close(): Cleans up resources

## Pipelined Execution\
- Parent operators process tuples as soon as child operators produce them
Benefits:
- No sync issues
- Avoids disk I/O for intermediate results
- Efficient resource use
- Preferred whenever possible
![pipe](/assets/unit6/pipe.png)

## Intermediate Tuple Materialization
- Stores operator results in temporary disk tables
- Adds overhead but is necessary when:
- An operator must reuse tuples multiple times
- Pipelining isn’t possible

## Measures of Query Cost
- Query cost reflects the resources used to execute a query. Traditionally focused on disk I/O, now shifting toward CPU due to SSDs and memory improvements. However, CPU cost is complex to estimate and often ignored.

1. Disk I/O Cost
- b: Number of blocks transferred
- S: Number of random accesses
- tT: Block transfer time
- tS: Seek time + rotational latency
- Cost formula: Cost = b × tT + S × tS

2. Typical I/O Times (2018 Data)
- Magnetic Disks
- tS ≈ 4 ms
- tT ≈ 0.1 ms
- SSDs (SATA)
- tS ≈ 90 µs
- tT ≈ 10 µs
- 4KB random reads ≈ 10,000/sec

3. Additional Considerations
- Writes are costlier than reads (especially on magnetic disks)
- SSD write speed ≈ 50% of read speed
- Buffer size affects I/O cost (e.g., PostgreSQL default: 4 GB)
- Writing query results not included in cost

4. Challenges in Estimating Response Time
- Depends on unknown buffer contents
- Affected by data layout and parallel disk access
- High-resource plans may be faster if run in parallel, but slower when run concurrently

5. Optimization Strategy
- Optimizers focus on minimizing total resource consumption
- This gives consistent performance across varied workloads
- Resource-based cost model is practical and widely used

## Selection Operation – Concise Summary
- Purpose: Retrieves records matching a condition during query processing.
### File Scan:
- Linear search through all blocks.
- Slow but always works.
### Using Indices
- Clustering Index: Fast for equality and range conditions on key.
- Secondary Index: Works for non-key attributes but can be slower
 especially with many matches.
- PostgreSQL Bitmap Index Scan: Efficient for uncertain result sizes.
### Complex Selections
- Conjunction (AND): Use composite indexes.
- Disjunction (OR): May need file scan if no index helps.
- Negation (NOT): Often requires full scan.
### Optimization Tips
- Choose the right index based on query type.
- Use composite indexes for complex conditions.
- Know when to fall back on file scan for performance.

## Sorting Operation – Concise Summary
- Importance: Organizes data for queries, joins, and other operations.
- External Sorting: Used when data doesn't fit in memory.
## External Sort-Merge Algorithm
### Stage 1: Create Sorted Runs
- Divide data into chunks that fit in memory.
- Sort each chunk and write to disk as a “run.”
### Stage 2: Merge Sorted Runs
- Use N-way merge to combine runs.
- Use buffers to minimize disk I/O.
### Cost Analysis
- Factors: Block transfers, disk seeks, buffer size.
- Goal: Minimize I/O by optimizing run size and merge steps.
### Significance
- Essential for processing large datasets.
- Improves performance of queries and joins.
- Understanding costs helps optimize database efficiency.

# Join Operation – Concise Summary
- Purpose: Combines data from multiple tables in relational databases.
- Types: Various join algorithms offer different trade-offs in efficiency.

1. Nested-Loop Join
- Method: For each tuple in Relation A, scan all tuples in Relation B.
- Costly: High computation due to examining every pair of tuples.
- Example: Joining student (5000 tuples, 100 blocks) with takes (10,000 tuples, 400 blocks) → very high cost.

2. Block Nested-Loop Join
- Improved Approach: Uses blocks instead of individual tuples.
- Efficiency: Fewer block accesses and lower I/O.
- Example: Same student and takes example.
### If fits in memory:
- Cost ≈ 900 block transfers (500 + 400) + 2 seeks.
- Benefit: Much faster and more efficient than basic nested-loop join.

# Join Algorithms
- Indexed Nested-Loop Join
- Uses index on join attribute for fast lookups.
- Reduces disk I/O compared to full scans.
### Merge Join
- Efficient for sorted relations.
- Merges like merge sort.
- Fast, but requires sorted inputs.

### Duplicate Elimination
- Removes duplicate tuples using sorting or hashing.
- Cost is similar to sort or hash cost.
### Projection
- Selects specific attributes.
- May produce duplicates — eliminate if needed.
- No elimination needed if attributes include a key.

### Set Operations
- (Union, Intersection, Difference)
- Implemented using sorting or hashing.
- Cost depends on input order and method used.

### Outer Join
- Includes unmatched tuples.
#### Implemented by:
- Post-processing after join, or
- Modifying join algorithm.
- Cost is similar to basic join operations.
- Aggregation (MIN, MAX, SUM, COUNT, AVG)
- Implemented via sorting or hashing on group attributes.
- Can use on-the-fly aggregation for efficiency.
- Expression Evaluation
### Materialization:
- Stores intermediate results in temporary tables.
- Evaluates step-by-step (bottom-up).
- Cost = operation costs + temp table I/O (write/read).

#### Example:
- br = nr / fr → blocks
- Seeks = ⌈br / bb⌉ (with bb = output buffer size)
- Pipelining: (not detailed here)
- Avoids writing intermediate results to disk.
- More efficient when applicable.

## Pipelining Approach
- Definition: Combine multiple operations into a sequence (pipeline), passing results directly between them.
- Goal: Avoid creating/reading temporary relations.
### Benefits:
- Reduces cost (no temp tables).
- Produces results earlier.
- Pipelining Implementation
- Demand-driven (Iterator-based):
- Parent pulls tuples via open, next, close.
- Producer-driven (Buffer-based):
- Child pushes tuples eagerly using buffers.

### Implementation options:
1. Combine operations into one.
2. Use iterators (demand-driven).
3. Use buffers (producer-driven).
- Pipeline Evaluation & Operators
- Blocking operators: Require full input (e.g., sort).
- Pipelineable operators: Work on inputs as they arrive (e.g., hash join, merge join).
- Stages: Group operations connected by pipelined edges.
## Pipelining for Data Streams
- For continuous input (e.g., sensor data).
- Use pipelined queries with tumbling windows for aggregation.
- Emit output when window closes.

## Cache-Conscious Algorithms
- Goal: Minimize cache misses, maximize cache hits.
- Why: CPU caches (L1, L2, L3) are much faster than main memory.
- Impact: Greatly improves performance for in-memory data.

## Cache-Conscious Sort
- Use external merge-sort with run sizes that fit in CPU cache.
- Sort each run in-memory to minimize cache misses.
- Merging is efficient due to sequential access.

## Cache-Conscious Hash Join
- Partition build relation to fit in cache with hash index.
- Perform build and probe phases per partition.
- For large relations, first partition to fit in memory, then apply cache-efficient join.

## Data Layout for Cache Efficiency
- Store frequently accessed attributes together.
- Example: Group-by and aggregate attributes side-by-side.
- Improves cache line utilization.

## Query Compilation
- Traditional: Interprets query plans (slower).
- Compiled: Converts plans to machine/byte code.
### Benefits:
- Pre-computes attribute offsets.
- Reduces overhead, up to 10x faster execution.

# What i have learned 
- From this unit, I learned how databases use indexes like ordered, hashed, and multilevel indices to speed up data retrieval. Ordered indices include clustering (primary) and non-clustering (secondary) types, with dense and sparse variations depending on how many entries are stored. B Trees and B+ Trees are essential for maintaining sorted and balanced structures for efficient querying, especially for large datasets. B+ Trees are widely used because all records are found at the leaf level and they support both sequential and random access efficiently. I also explored how hash indices support equality-based lookups using hash functions and are commonly used in joins or in-memory operations.

- I also learned about query processing, which transforms SQL queries into efficient execution plans through parsing, optimization, and evaluation. Query execution may use pipelining to reduce I/O by passing data directly between operations. Techniques like external sorting, joins (nested-loop, merge, hash), projection, and aggregation were examined for their performance impacts. Advanced strategies such as cache-conscious algorithms improve in-memory performance by aligning operations with CPU cache behavior. Finally, query optimization balances disk I/O, CPU use, and memory layout to execute queries effectively, sometimes even compiling them to machine code for better speed.

## THE END





















